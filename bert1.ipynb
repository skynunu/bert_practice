{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"bert1.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1H3xkdI9Ashzxs1j78aG0Z6mk75L9VQ4_","authorship_tag":"ABX9TyMqlJSv91mFJ7BdmWIvaZZt"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"SBVDrcyhfzBt"},"source":["import os\r\n","import re\r\n","import numpy as np\r\n","from tqdm import tqdm\r\n","\r\n","import tensorflow as tf\r\n","from transformers import *\r\n","\r\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\r\n","from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\r\n","\r\n","import pandas as pd\r\n","import matplotlib.pyplot as plt\r\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GPK4ttM_z9k2","executionInfo":{"status":"ok","timestamp":1611817555394,"user_tz":-540,"elapsed":3105,"user":{"displayName":"최종찬","photoUrl":"","userId":"12659814370838060539"}},"outputId":"2c5fb5f1-0092-41dc-8111-9430fc60a78b"},"source":["pip install transformers"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (4.2.2)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.19.5)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.8)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from transformers) (3.4.0)\n","Requirement already satisfied: tokenizers==0.9.4 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.9.4)\n","Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.12.5)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.0.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.0)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"QfHjzp4tzoMw"},"source":["from transformers import *\r\n","tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\r\n","#다국어 지원을 위한 BerrTokenizer 모델 사용, wordpiece 모델을 사용해서 학습을 진행한 것 "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QrGBo14NrGrr","executionInfo":{"status":"ok","timestamp":1611817707190,"user_tz":-540,"elapsed":874,"user":{"displayName":"최종찬","photoUrl":"","userId":"12659814370838060539"}},"outputId":"a3828825-a9ce-4429-cab1-d3991c51329f"},"source":["print(tokenizer.all_special_tokens,\"\\n\", tokenizer.all_special_ids)\r\n","#BertTokenizer에서 사용되는 스페셜 토큰의 종류와 인덱스"],"execution_count":null,"outputs":[{"output_type":"stream","text":["['[UNK]', '[SEP]', '[PAD]', '[CLS]', '[MASK]'] \n"," [100, 102, 0, 101, 103]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"EoFm1800reAo","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1611817813627,"user_tz":-540,"elapsed":1144,"user":{"displayName":"최종찬","photoUrl":"","userId":"12659814370838060539"}},"outputId":"ec907f17-7a0d-414b-bb69-ccfafaf5c0cd"},"source":["kor_encode = tokenizer.encode(\"안녕하세요. 반갑습니다.\")\r\n","kor_decode = tokenizer.decode(kor_encode)\r\n","print(\"안녕하세요. 반갑습니다.\")\r\n","print(kor_encode,\" / \", kor_decode) "],"execution_count":null,"outputs":[{"output_type":"stream","text":["안녕하세요. 반갑습니다.\n","[101, 9521, 118741, 35506, 24982, 48549, 119, 9321, 118610, 119081, 48345, 119, 102]  /  [CLS] 안녕하세요. 반갑습니다. [SEP]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"sma1QN-AxH1m"},"source":[""]},{"cell_type":"code","metadata":{"id":"fDiUPHGCzGyO"},"source":["#random seed 고정\r\n","tf.random.set_seed(1234)\r\n","np.random.seed(1234)\r\n","\r\n","BATCH_SIZE = 32\r\n","NUM_EPOCHS = 3\r\n","VALID_SPLIT = 0.2\r\n","MAX_LEN = 39 # EDA에서 추출된 Max Length\r\n","DATA_IN_PATH = \"/content/drive/MyDrive/Colab Notebooks/nlp book/data_in/KOR\"\r\n","DATA_OUT_PATH = \"/content/drive/MyDrive/Colab Notebooks/nlp book\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DDr3jXkwyENb"},"source":["# Korean Movie Review Classification"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":230},"id":"LVaOpw6kv48D","executionInfo":{"status":"ok","timestamp":1611817852016,"user_tz":-540,"elapsed":3646,"user":{"displayName":"최종찬","photoUrl":"","userId":"12659814370838060539"}},"outputId":"9999cb4f-56cf-4068-c213-910c14438461"},"source":["# 데이터 전처리 준비\r\n","# 15만개의 네이버 영화 리뷰 데이터 프레임 train_data\r\n","DATA_TRAIN_PATH = os.path.join(DATA_IN_PATH, \"naver_movie\", \"ratings_train.txt\")\r\n","\r\n","train_data = pd.read_csv(DATA_TRAIN_PATH, header = 0, delimiter = '\\t', quoting = 3)\r\n","print(len(train_data))\r\n","train_data = train_data.dropna()\r\n","train_data.head()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["150000\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>document</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>9976970</td>\n","      <td>아 더빙.. 진짜 짜증나네요 목소리</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>3819312</td>\n","      <td>흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>10265843</td>\n","      <td>너무재밓었다그래서보는것을추천한다</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>9045019</td>\n","      <td>교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>6483659</td>\n","      <td>사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["         id                                           document  label\n","0   9976970                                아 더빙.. 진짜 짜증나네요 목소리      0\n","1   3819312                  흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나      1\n","2  10265843                                  너무재밓었다그래서보는것을추천한다      0\n","3   9045019                      교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정      0\n","4   6483659  사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...      1"]},"metadata":{"tags":[]},"execution_count":17}]},{"cell_type":"markdown","metadata":{"id":"1D37gRpa1mLd"},"source":["### 세가지 유형의 input값\r\n","\r\n","-input_ids : 문장을 토크나이즈해서 인덱스 값으로 변환한 값\r\n","\r\n","-attention_mask : 패딩된 부분에 대해서, 학습에 영향을 받지 않게 처리해주는 입력값, 실질적인 값이 존재하면 1,  패딩된부분은 0\r\n","\r\n","-token_type_ids : 두개의 시퀀스를 입력으로 활용할 대 0과 1로 문장의 토큰 값을 분리"]},{"cell_type":"code","metadata":{"id":"X6Qt6NkkyQOv","executionInfo":{"status":"ok","timestamp":1611832824352,"user_tz":-540,"elapsed":2021,"user":{"displayName":"최종찬","photoUrl":"","userId":"12659814370838060539"}}},"source":["# Bert Tokenizer \r\n","def bert_tokenizer(sent, MAX_LEN):\r\n","    \r\n","    encoded_dict = tokenizer.encode_plus(\r\n","        text = sent,\r\n","        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\r\n","        max_length = MAX_LEN,      # Pad & truncate all sentences.\r\n","        pad_to_max_length = True,\r\n","        return_attention_mask = True   # Construct attn. masks.       \r\n","    )\r\n","    \r\n","    input_id = encoded_dict['input_ids'] # encode한것\r\n","    attention_mask = encoded_dict['attention_mask'] # And its attention mask (simply differentiates padding from non-padding).\r\n","    token_type_id = encoded_dict['token_type_ids'] # differentiate two sentences\r\n","    \r\n","    return input_id, attention_mask, token_type_id"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"9phaW_4isrWv","executionInfo":{"status":"ok","timestamp":1611832824357,"user_tz":-540,"elapsed":1998,"user":{"displayName":"최종찬","photoUrl":"","userId":"12659814370838060539"}}},"source":["# 정규표현식을 활용해서 한글이외의 특수문자를 제외해주는 함수\r\n","def clean_text(sent) : \r\n","  sent_clean = re.sub(\"[^가-힣 ㄱ-ㅎ ㅏ-ㅣ\\\\s]\", ' ', sent)\r\n","  return sent_clean"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":241},"id":"DMOIkzfHtUzC","executionInfo":{"status":"error","timestamp":1611832824358,"user_tz":-540,"elapsed":1939,"user":{"displayName":"최종찬","photoUrl":"","userId":"12659814370838060539"}},"outputId":"cb8d50cb-8bbf-4892-c63e-a6c9fd1a49ea"},"source":["input_ids = []\r\n","attention_masks = []\r\n","token_type_ids = []\r\n","train_data_labels = []\r\n","\r\n","\r\n","for train_set, train_label in zip(train_data['document'], train_data['label']) :\r\n","  try:\r\n","    input_id, attention_mask, token_type_id = bert_tokenizer(clean_text(train_set),MAX_LEN) #MAX_LEN=39\r\n","    \r\n","    #각각의 리스트에 추가해주기\r\n","    input_ids.append(input_id)\r\n","    attention_masks.append(attention_mask)\r\n","    token_type_ids.append(token_type_id)\r\n","    train_data_labels.append(train_label)\r\n","  except Exception as e :\r\n","    print(e)\r\n","    print(train_set)\r\n","    pass"],"execution_count":3,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-25e53f184a5b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mtrain_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_label\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'document'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0minput_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_type_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbert_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclean_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mMAX_LEN\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#MAX_LEN=39\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'train_data' is not defined"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-BJxMAPWybDf","executionInfo":{"status":"ok","timestamp":1611818345465,"user_tz":-540,"elapsed":2364,"user":{"displayName":"최종찬","photoUrl":"","userId":"12659814370838060539"}},"outputId":"c68e866f-1051-46a2-d9fb-7706907c139a"},"source":["#위의 리스트들을 넘파이로 바꿔주기\r\n","train_movie_input_ids = np.array(input_ids, dtype=int)\r\n","train_movie_attention_masks = np.array(attention_masks, dtype=int)\r\n","train_movie_type_ids = np.array(token_type_ids, dtype=int)\r\n","\r\n","train_movie_inputs = (train_movie_input_ids, train_movie_attention_masks, train_movie_type_ids)\r\n","train_data_labels = np.asarray(train_data_labels, dtype=np.int32) \r\n","\r\n","print(\"# sents: {}, # labels: {}\".format(len(train_movie_input_ids), len(train_data_labels)))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["# sents: 149995, # labels: 149995\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eRGhDOAX4B5r","executionInfo":{"status":"ok","timestamp":1611818504330,"user_tz":-540,"elapsed":798,"user":{"displayName":"최종찬","photoUrl":"","userId":"12659814370838060539"}},"outputId":"644d1a47-b9e2-43a4-d4c1-1133e3ae8194"},"source":["# 최대 길이: 39\r\n","input_id = train_movie_input_ids[1]\r\n","attention_mask = train_movie_attention_masks[1]\r\n","token_type_id = train_movie_type_ids[1]\r\n","\r\n","print(\"문장 \", train_data.iloc[1]['document'])\r\n","print(tokenizer.decode(input_id))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["문장  흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나\n","[CLS] [UNK] 포스터보고 초딩영화줄 오버연기조차 가볍지 않구나 [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DiOgxTzB3kNv","executionInfo":{"status":"ok","timestamp":1611818508489,"user_tz":-540,"elapsed":720,"user":{"displayName":"최종찬","photoUrl":"","userId":"12659814370838060539"}},"outputId":"d5638bda-6eff-4459-8124-a3eb3de15846"},"source":["print(\"input_id\", input_id)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["input_id [   101    100   9928  58823  30005  11664   9757 118823  30858  18227\n"," 119219   9580  41605  25486  12310  20626  23466   8843 118986  12508\n","   9523  17196  16439    102      0      0      0      0      0      0\n","      0      0      0      0      0      0      0      0      0]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3LuAXwNN3mI4","executionInfo":{"status":"ok","timestamp":1611818516156,"user_tz":-540,"elapsed":798,"user":{"displayName":"최종찬","photoUrl":"","userId":"12659814370838060539"}},"outputId":"dfa33f3a-6b95-44a4-d239-9469422c0f16"},"source":["print(\"attention_mask\", attention_mask)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["attention_mask [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0\n"," 0 0]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"muxXcxBZ3mQF","executionInfo":{"status":"ok","timestamp":1611818518028,"user_tz":-540,"elapsed":848,"user":{"displayName":"최종찬","photoUrl":"","userId":"12659814370838060539"}},"outputId":"9fa98d97-eab7-4a01-cb1e-f026a59c42cc"},"source":["print(\"token_type_id\", token_type_id)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["token_type_id [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"," 0 0]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"mRJ1IpLp81w2"},"source":["import keras"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9TzZABi_4sLz","executionInfo":{"status":"ok","timestamp":1611820053331,"user_tz":-540,"elapsed":2561,"user":{"displayName":"최종찬","photoUrl":"","userId":"12659814370838060539"}},"outputId":"894096ca-a78e-42e2-e6db-9020fa23f496"},"source":["drive_dir_path ='/content/drive/MyDrive/Colab Notebooks/nlp book/bert_ckpt'\r\n","\r\n","# bert를 활용한 한국어 텍스트 분류 모델 만들기\r\n","class TFBertClassifier(tf.keras.Model) :\r\n","  def __init__ (self, model_name, dir_path, num_class):\r\n","    super(TFBertClassifier,self).__init__()\r\n","\r\n","    self.bert = TFBertModel.from_pretrained(model_name, cache_dir= dir_path ) \r\n","    # 기존에 사전학습된 모델의 가중치 부분들이 로드된다.\r\n","    \r\n","    self.dropout = tf.keras.layers.Dropout(self.bert.config.hidden_dropout_prob)\r\n","    # self.bert.config.hidden_dropout_prob = 0.1\r\n","\r\n","    self.classifier = tf.keras.layers.Dense(num_class, kernel_initializer=tf.keras.initializers.TruncatedNormal(\r\n","      self.bert.config.initializer_range), name='classifier')\r\n","    # num_class에 원하는 분류 값을 추가해서 정답의 개수를 정할 수 있다.\r\n","    # tf.keras.initializers.TruncatedNormal는 절단 정규 분포로부터 무작위 값을 선택, 대신 선택범위를 평균으로 2표준편차 안쪽으로 제한한다.\r\n","    # self.bert.config.initializer_range =0.02\r\n","    \r\n","\r\n","  def call (self, inputs, attention_mask = None, token_type_ids = None, training = Fasle ):\r\n","    outputs= self.bert(inputs, attention_mask=attention_mask, token_type_ids = token_type_ids)   \r\n","    #outputs 값: # sequence_output, pooled_output, (hidden_states), (attentions)이 된다\r\n","    pooled_output = outputs[1] # \r\n","    pooled_output = self.dropout(pooled_output, training=training)\r\n","    logits = self.classifier(pooled_output) # 마지막 출력에 완전연결층 1층 적용, 필요하다면 여러개 층을 추가 가능하다.\r\n","    return logits\r\n","\r\n","cls_model = TFBertClassifier(model_name='bert-base-multilingual-cased',\r\n","                                  dir_path=drive_dir_path,\r\n","                                  num_class=2)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Some layers from the model checkpoint at bert-base-multilingual-cased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n","- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","All the layers of TFBertModel were initialized from the model checkpoint at bert-base-multilingual-cased.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6xF8j_wA6f05","executionInfo":{"status":"ok","timestamp":1611820615759,"user_tz":-540,"elapsed":2608,"user":{"displayName":"최종찬","photoUrl":"","userId":"12659814370838060539"}},"outputId":"d5f0564a-707f-4542-ed54-c5a28fbdc567"},"source":["drive_dir_path ='/content/drive/MyDrive/Colab Notebooks/nlp book/bert_ckpt'\r\n","bert = TFBertModel.from_pretrained('bert-base-multilingual-cased',\r\n","                                  cache_dir=drive_dir_path,)\r\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Some layers from the model checkpoint at bert-base-multilingual-cased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n","- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","All the layers of TFBertModel were initialized from the model checkpoint at bert-base-multilingual-cased.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"WOMOREWI_LkE"},"source":["output = bert(train_movie_input_ids[:5], train_movie_attention_masks[:5], train_movie_type_ids[:5])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4pZlefqk6ULl"},"source":["# 모델 컴파일\r\n","optimizer = tf.keras.optimizers.Adam(3e-5)\r\n","loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\r\n","metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\r\n","cls_model.compile(optimizer=optimizer, loss=loss, metrics=[metric])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UlYehk3_9IZW","outputId":"6f8d2320-64ec-49d4-a350-7299ee9cb32c"},"source":["model_name = \"tf2_bert_naver_movie\"\r\n","earlystop_callback = EarlyStopping(monitor='val_accuracy', min_delta=0.0001,patience=2)\r\n","\r\n","checkpoint_path = os.path.join(DATA_OUT_PATH, model_name, 'weights.h5')\r\n","checkpoint_dir = os.path.dirname(checkpoint_path)\r\n","\r\n","# Create path if exists\r\n","if os.path.exists(checkpoint_dir):\r\n","    print(\"{} -- Folder already exists \\n\".format(checkpoint_dir))\r\n","else:\r\n","    os.makedirs(checkpoint_dir, exist_ok=True)\r\n","    print(\"{} -- Folder create complete \\n\".format(checkpoint_dir))\r\n","\r\n","cp_callback = ModelCheckpoint(\r\n","    checkpoint_path, monitor='val_accuracy', verbose=1, save_best_only=True, save_weights_only=True)\r\n","\r\n","# 학습과 eval 시작\r\n","history = cls_model.fit(train_movie_inputs, train_data_labels, epochs=NUM_EPOCHS, batch_size=BATCH_SIZE,\r\n","                    validation_split = VALID_SPLIT, callbacks=[earlystop_callback, cp_callback])\r\n","\r\n","#steps_for_epoch\r\n","\r\n","print(history.history)\r\n","\r\n","#코랩에서 gpu를 사용해 epoch 3를 주었는데도 거의 3시간 가량이 걸렸다."],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/drive/MyDrive/Colab Notebooks/nlp book/tf2_bert_naver_movie -- Folder create complete \n","\n","Epoch 1/3\n","WARNING:tensorflow:AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7f2266870590>> and will run it as-is.\n","Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n","Cause: <cyfunction Socket.send at 0x7f2284217d90> is not a module, class, method, function, traceback, frame, or code object\n","To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n","WARNING: AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7f2266870590>> and will run it as-is.\n","Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n","Cause: <cyfunction Socket.send at 0x7f2284217d90> is not a module, class, method, function, traceback, frame, or code object\n","To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"],"name":"stdout"},{"output_type":"stream","text":["The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING:tensorflow:AutoGraph could not transform <function wrap at 0x7f2281ba49d8> and will run it as-is.\n","Cause: while/else statement not yet supported\n","To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"],"name":"stdout"},{"output_type":"stream","text":["The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING: AutoGraph could not transform <function wrap at 0x7f2281ba49d8> and will run it as-is.\n","Cause: while/else statement not yet supported\n","To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"],"name":"stdout"},{"output_type":"stream","text":["The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n","The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"],"name":"stderr"},{"output_type":"stream","text":["3750/3750 [==============================] - ETA: 0s - loss: 0.4849 - accuracy: 0.7512"],"name":"stdout"},{"output_type":"stream","text":["The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n","The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"],"name":"stderr"},{"output_type":"stream","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r3750/3750 [==============================] - 1245s 323ms/step - loss: 0.4849 - accuracy: 0.7512 - val_loss: 0.3630 - val_accuracy: 0.8380\n","\n","Epoch 00001: val_accuracy improved from -inf to 0.83799, saving model to /content/drive/MyDrive/Colab Notebooks/nlp book/tf2_bert_naver_movie/weights.h5\n","Epoch 2/3\n","  63/3750 [..............................] - ETA: 18:33 - loss: 0.3399 - accuracy: 0.8393"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"4Rbv6Rt8IFWz"},"source":["DATA_TEST_PATH = os.path.join(DATA_IN_PATH, \"naver_movie\", \"ratings_test.txt\")\r\n","test_data = pd.read_csv(DATA_TEST_PATH, header = 0, delimiter = '\\t', quoting = 3)\r\n","test_data = test_data.dropna()\r\n","test_data.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cA1GP1Y079U8"},"source":["input_ids = []\r\n","attention_masks = []\r\n","token_type_ids = []\r\n","test_data_labels = []\r\n","\r\n","for test_sent, test_label in tqdm(zip(test_data[\"document\"], test_data[\"label\"])):\r\n","    try:\r\n","        input_id, attention_mask, token_type_id = bert_tokenizer(test_sent, MAX_LEN)\r\n","\r\n","        input_ids.append(input_id)\r\n","        attention_masks.append(attention_mask)\r\n","        token_type_ids.append(token_type_id)\r\n","        test_data_labels.append(test_label)\r\n","    except Exception as e:\r\n","        print(e)\r\n","        print(test_sent)\r\n","        pass\r\n","\r\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VEC0FL9sJDAx"},"source":["test_movie_input_ids = np.array(input_ids, dtype=int)\r\n","test_movie_attention_masks = np.array(attention_masks, dtype=int)\r\n","test_movie_type_ids = np.array(token_type_ids, dtype=int)\r\n","test_movie_inputs = (test_movie_input_ids, test_movie_attention_masks, test_movie_type_ids)\r\n","\r\n","test_data_labels = np.asarray(test_data_labels, dtype=np.int32) #레이블 토크나이징 리스트\r\n","\r\n","print(\"num sents, labels {}, {}\".format(len(test_movie_input_ids), len(test_data_labels)))\r\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xAtvE4psJPbS"},"source":["results = cls_model.evaluate(test_movie_inputs, test_data_labels, batch_size=1024)\r\n","print(\"test loss, test acc: \", results)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4NVn3N0KKqFn"},"source":["추가로 성능을 올리기 위한 방법으로 전처리 및 다양한 변수를 변형해가며, 변화시키는 방법\r\n","\r\n","다국어가 아닌 한국어로 사전학습한 모델을 활용해 미세 조정하는 방법등이 있다."]},{"cell_type":"code","metadata":{"id":"Zl4fFZWJKpcJ"},"source":[""],"execution_count":null,"outputs":[]}]}